\section{Technology Assessment}
\label{sec:technology}


(Remove later: Introduce in sufficient depth the key concepts and architecture of the chosen software technology. As part of this, you may consider using a running example to introduce the technology.)


At its core, Apache Kafka is a distributed event streaming platform. Think of it
as a highly scalable, fault-tolerant, and durable "log file" that different parts
of an application can write to and read from simultaneously. The \texttt{FeedApp}
project uses these core concepts to decouple tasks. When a user votes, the
system does not immediately try to update every other user's screen. Instead, it
publishes an "event" to Kafka, and other services (like the WebSocket
broadcaster) react to that event.

\subsubsection*{Event:}
An event is the most basic unit of data, representing the
fact that "something happened."
In the \texttt{FeedApp}, an event is a
\texttt{Map<String, Object>} that gets serialized into JSON. For example, when
a poll is created, the \texttt{PollEventListener} creates an event with the
\texttt{pollId}, \texttt{question}, and \texttt{eventType}.
The consumer listens for vote change events, which it receives
as a \texttt{Map} containing the \texttt{pollId} and \texttt{optionOrder}.

\subsubsection*{Topic:}
A topic is a named "category" or "feed" where events are
stored and published. A cluster can host many topics, and with KRaft
mode, the number of topics can now scale into the millions \cite{kafka_topics}.
The \texttt{FeedApp} uses a dynamic topic strategy.
Instead of one giant \texttt{votes} topic, the \texttt{PollTopicManager}
creates a unique topic for every single poll.
For example, for a poll with ID \texttt{10}, the topic name would be
\texttt{poll.voteChange.10}.

\subsubsection*{Producer:}
A producer is a client application that writes
events to a Kafka topic.
The \texttt{ProducerService} in the FeedApp project is a classic producer,
using Spring's \texttt{KafkaTemplate} to send events.
This service is used by the \texttt{PollEventListener} to send a message
after a new poll is successfully saved to the database.
With the 3-node cluster, the producer (our backend) is given a list
of all three brokers (\texttt{kafka-1:29092,kafka-2:29092,kafka-3:29092})
via its environment variables. The Kafka
client uses this list to discover the entire cluster.

\subsubsection*{Consumer:}
A consumer is a client application that reads (subscribes
to) events from one or more Kafka topics.
The \texttt{ConsumerService} acts as the consumer
.
It uses a powerful feature to match the dynamic topic strategy: it listens to a
\texttt{topicPattern} (\texttt{"poll.voteChange.*"}) instead of a fixed topic
name. This allows it to automatically discover and consume from new poll topics
as soon as they are created.

\subsubsection*{Consumer Group:}
A set of consumers that work together to process events
from topics. Kafka guarantees that each event in a topic's partition is
delivered to only one consumer instance within that group.
The \texttt{FeedApp} defines the consumer group ID
as \texttt{"poll-app"} in its \texttt{application.\allowbreak properties} file.
If one were to run multiple instances of the backend service for scalability,
they would all share this group ID. Kafka would then automatically balance the
load of all the poll topics among them.

\subsubsection*{Partition:}
{\sloppy
Partitions are the core of Kafka's scalability. A topic is
split into one or more partitions. Each partition is an ordered, immutable log
of events, which can be hosted on different brokers.
The \texttt{FeedApp} project makes a specific design
choice. When the \texttt{PollTopicManager} creates a new topic, it is
configured with one partition to guarantee strict event ordering for
that poll. To match the 3-node cluster's fault-tolerant
design, the replication factor is set to 3. This is implemented
by calling \texttt{new NewTopic} with the parameters
\texttt{(topicName, 1, (short) 3)}.
This configuration ensures the single partition is copied to all three brokers,
providing fault tolerance and high availability.
}

\subsubsection*{Broker:}
A broker is a single Kafka server. Brokers receive messages
from producers, assign offsets to them, and commit them to the partition log on
disk, which provides Kafka's durability.
The provided \texttt{docker-compose.yml}
file defines three distinct broker services: \texttt{kafka-1}, \texttt{kafka-2},
and \texttt{kafka-3}. Each of these nodes
is configured with the \texttt{broker} role.

\subsubsection*{Cluster:}
A Kafka cluster is a group of brokers working together to
provide scalability, availability, and fault tolerance. Partitions are
replicated on multiple brokers based on the topic's replication factor.
The three brokers (\texttt{kafka-1},
\texttt{kafka-2}, \texttt{kafka-3}) form the cluster.
This is where the concept of a replication factor of 3 becomes reality.
When \texttt{PollTopicManager} creates \texttt{poll.voteChange.10} with 3
replicas, that topic's single partition is copied to all three brokers. One
broker is elected "leader" for that partition (handling all writes), while the
other two are "followers." If the leader fails, a follower is
promoted, ensuring no data loss. This entire process is managed by the KRaft
controller quorum, which in this 3-node setup consists of all three nodes
(\texttt{KAFKA\_CONTROLLER\_QUORUM\_VOTERS: '1@kafka-1:9093,...'}).

This part of the report aims to follow this software technology evaluation framework. ~\ref{fig:framework} from \cite{brown:96}


\begin{figure}[thb]
	\centering
	\includegraphics[scale=0.5]{figs/framework.png}
	\caption{Software technology evaluation framework.}
	\label{fig:framework}
\end{figure}

\subsection{Descriptive Modeling}

Remove later: write where the technology comes from, its history, its context and what problem it solves.
Consider drawing a graph like in \cite{brown:96}.

The purpose of this phase is to establish a formal model for evaluating two candidate technologies, Apache Kafka (in KRaft mode) and RabbitMQ for adoption within our project.
First, we will define the Technology Domain Genealogy for both candidates.
This model will establish Kafka (KRaft) as an evolution to its "precursor" (Kafka with ZooKeeper), explaining the problems it was designed to solve.
It will also define RabbitMQ's distinct architectural heritage.
Second, we will define our project's Problem Domain Habitat, the specific usage contexts and project needs, to determine which candidate is a better fit.

\subsubsection{Technology Domain Genealogy}
This model describes the ancestry and core design philosophy of the two competing technologies.
Candidate technology 1 is Apache Kafka (KRaft mode). Kafka is, as stated earlier, a distributed event streaming platform. Its core architecture is a persistent, immutable log.
It uses a "dumb broker/smart consumer" model, where the consumers "pull" data by tracking their position using an offset within the log.
The KRaft mode we are looking at is a significant evolution. Its "precursor" architecture required a separate, external Apache ZooKeeper cluster.
This external dependency was known to cause:

1. High operational complexity: Requiring the management and security of two separate distributed systems.

2. Scalabiliy bottlenecks: Limiting clusters to tens of thousands of partitions.

3. Slow recovery times: Controller failover and metadata loading were slow.

KRaft replaces ZooKeeper by integrating a consensus protocol inside Kafka.
This was designed to provide simpler operations, massive scalability (to millions of partitions), and near instant, sub-second failover.

Candidate technology 2 is RabbitMQ.
RabbitMQ is a traditional message broker with a long heritage rooted in reliable delivey protocols like AMQP.
It functions as a "smart broker/dumb consumer" platform. The smart broker manages complex, flexible routing via exchanges and actively pushes messages to consumers.
Its core data structure is a queue. Messages are stateful and are typically deleted from the queue once successfully consumed and acknowledged.

\subsubsection{Problem Domain Habitat}
As defined by the evaluation framework, the "Problem Domain Habitat" describes the specific usage contexts, problem characteristics, and key requirements where the candidate technologies will be deployed.
Our habitat is the FeedApp backend, a real-time, interactive polling application built on Spring Boot and Java.
The primary objectives for this habitat are to find a messaging solution that is highly scalable, supports high throughput, and simultaneously provides good (low) latency.
These objetives can be split into two separate problem domain habitats.

Habitat 1: High-throughput event ingestion

The application must reliably ingest a high volume of events, such as poll creation, comments, and most critically votes from a potentially large and concurrent user base.
A user action is sent via the web API, processed, and then published as an immutable event by a producer service.
There are a few key requirements to this problem habitat.
We prioritize high throughput (handling thousands of writes per second) and scalability (the ability to add more nodes to handle increased load). 
The durability of these events is critical, as they effecitvely form the application's historical log.

Habitat 2: Real-time asynchronous fan-out

The application must feel responsive and as if event processing happens in real-time, as this is important for a good user experience.
When a poll-related event is successfully processed, the system must immediately notift all subscribed clients.
A consumer service listens for new events from a topic, it triggers a notification service, which pushes updates to clients via WebSockets.
In this habitat we prioritize low end-to-end latency. The time from an events publication to its consuption must be minimal for the application to feel responsive.

\subsection{Experiment Design}

Remove later: Write you hypotheses about what benefits the technology bring and how you can support or reject them via experiments.

Following the descriptive modeling phase, we now design the experiments to empirically evaulate our candidate technologies.
The goal of this phase is to formulate refutable hypotheses based on our problem domain habitat and design experiments to generate quantitative data to test them.

Our design focuses on getting objective data for our key requirements: throughput, latency and scalability.

\subsubsection{Hypothesis Formulation}
Our hypotheses are derived directly from the two habitats identified in the FeedApp backend and they key feature deltas between Kafka, which is a log based pull model, and RabbitMQ which is a queue based, push model.

Habitat 1 prioritizes high throughput and scalability.

Hypothesis 1.A (Throughput): In a many producers, high volume workload, Kafka will achieve significantly higher maximum throughput than RabbitMQ before latency thresholds are breadched.
Here, throughput will be measured as messages per second.

Hypothesis 1.B (Scalability): As the number of producers increases, Kafka's throughput will scale better than RabbitMQ's.
Here, better scaling will be how much more load can be handled when increasing cluster size.

Habitat 2 prioritizes low end-to-end latency.

Hypothesis 2.A (Low-load latency): In a low message rate workload, RabbitMQ's push model will demonstrate lower p99 end-to-end latency than Kafka's pull model.

Hypothesis 2.B (High-load latency): As the message rate in this habitat increases, Kafka's pull model will become more efficient, and its average latency will become lower than RabbitMQ's, which will suffer from per-message overhead.

\subsubsection{Experiment Design and Workloads}
To test these hypotheses, we will use the OpenMessaging Benchmark (OMB) framework.
This provides a standardized, vendor-neutral toolset to conduct repeatable performance tests easily.
We will configure OMB with two distinct workloads, each designed as a model problem that simulates one of our defined habitats.

Workload 1 for Habitat 1: Ingestion test

The purpose of this workload is to test hypotheses 1.A and 1.B.
This will be the OMB configuration for this test.
"INSERT WORKLOAD FILE when i get home"
We will have many producers, a single topic/exchange, and a low number of consumers.
This simulates thousands of concurrent users casting votes and creating polls in the FeedApp.
There are three metrics we wish to capture in this test.

1. Maximum throughput: The highest sustainable message rate each platform can ingest

2. Producer latency (p99): The 99th percentile latency for a producer to successfully publish a message.

3. Scalability curve: Run the test with 1,5 and 10 producer instances to observe how throughput scales.

Workload 2 for Habitat 2: Fan-out test

The purpose of this workload is to test hypotheses 2.A and 2.B.
The will be the OMB configuration for the test.
"INSERT WORKLOAD FILE when i get home"
We will have a single producer and many competing consumers on a shared subscription.
This simulates real time fanout of poll updates to all connected clients.
There are two metrics we wish to capture in this test:

1. End-to-end latency (p99): The time from message publish to message receipt by a consumer.

2. Latency vs. Throughput Curve: We will specifically run this workload at varying throughput rates (1000 msg/sec, 10,000 msg/sec and 100,000 msg/sec) to find the crossover point predicted in 2.B.

\subsubsection{Comparative Feature Analysis}
We acknowledge that our locally run benchmarks will be limited by our specific hardware and test duration. 
Therefore, as part of our comparative feature analysis, we will supplement out primary findings with a review of existing, larger scale benchmarks published by others.
This aims to validate our results and provide a broader context for the performance characteristics that we cannot easily reproduce.

\subsection{Experiment Evaluation}

Write about the results of your experiments, either via personal experience reports, quantitative benchmarks, a demostrator case study or a combination of multiple approaches.


For some reports you may have to include a table with experimental
results are other kinds of tables that for instance compares
technologies. Table~\ref{tab:results} gives an example of how to create a table.

\begin{table}[bth]
	\centering
	\begin{tabular}{llrrrrrr}
		Config & Property & States & Edges & Peak & E-Time & C-Time & T-Time
		\\ \hline \hline
		22-2 & A   &    7,944  &   22,419  &  6.6  \%  &  7 ms & 42.9\% &  485.7\% \\
		22-2 & A   &    7,944  &   22,419  &  6.6  \%  &  7 ms & 42.9\% &  471.4\% \\
		30-2 & B   &   14,672  &   41,611  &  4.9  \%  & 14 ms & 42.9\% &  464.3\% \\
		30-2 & C   &   14,672  &   41,611  &  4.9  \%  & 15 ms & 40.0\% &  420.0\% \\ \hline
		10-3 & D   &   24,052  &   98,671  & 19.8  \%  & 35 ms & 31.4\% &  285.7\% \\
		10-3 & E   &   24,052  &   98,671  & 19.8  \%  & 35 ms & 34.3\% &  308.6\% \\
		\hline \hline
	\end{tabular}
	\caption{Selected experimental results on the communication protocol example.}
	\label{tab:results}
\end{table}
