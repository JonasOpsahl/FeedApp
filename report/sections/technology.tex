\section{Technology Assessment}
\label{sec:technology}


At its core, Apache Kafka is a distributed event streaming platform acting as a fault-tolerant log.
The \textbf{FeedApp} uses Kafka to decouple tasks: user votes publish events that trigger asynchronous reactions from services (like the WebSocket broadcaster) rather than blocking immediate updates.

\paragraph{Event: }
An event is the most basic unit of data representing a state change. In FeedApp, a `Vote Cast' is an event.
It is not just a database update; it is an immutable record containing the \texttt{pollId} and the option selected, which triggers reactions across the system.

\paragraph{Topic: }
A topic is a category where events are stored.
FeedApp utilizes a `dynamic topic' strategy where every unique poll gets its own topic (e.g., \texttt{poll.voteChange.10}).
This ensures that events for a specific poll are isolated, allowing consumers to subscribe only to active polls rather than filtering through a monolithic stream of all system data.


\paragraph{Producer: }
A producer is the client application that writes events to the log.
In our architecture, the Spring Boot backend acts as the producer.
When a user submits a vote via the REST API, the producer publishes this event to the cluster, decoupling the immediate user request from the downstream processing.


\paragraph{Consumer and Consumer Group: }
Consumers read events from topics.
In FeedApp, the WebSocket service acts as a consumer.
It listens to vote events to broadcast real-time updates to the frontend.
By placing these consumers in a \textbf{Consumer Group} (configured as poll-app), Kafka ensures that if we scale the backend to multiple instances, the processing of poll topics is automatically load-balanced among them.

\paragraph{Partition and Replication: }
Partitions allow topics to be split across multiple brokers for scalability, while replication ensures fault tolerance.
For \textbf{FeedApp}, we configure our poll topics with a single partition to guarantee strict ordering of votes (ensuring the vote count is always accurate sequentially) but replicate that partition across three nodes.
This ensures that even if one broker fails, the poll data remains available.


\paragraph{Broker and Cluster: }
A broker is a single Kafka server, and a cluster is a group of brokers working together.
Our setup uses a three-node cluster \texttt{(kafka-1, kafka-2, kafka-3)} managed by the KRaft consensus protocol\cite{KRaft_mode}.
This mimics a production environment where the system can survive hardware failures without data loss.

This part of the report aims to follow this software technology evaluation framework.~\ref{fig:framework}


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figs/framework.png}
	\caption{Software technology evaluation framework.}
	\label{fig:framework}
\end{figure}

\subsection{Descriptive Modeling}

The purpose of this phase is to establish a formal model for evaluating two candidate technologies, Apache Kafka (in KRaft mode) and RabbitMQ for adoption within our project.
First, we will define the Technology Domain Genealogy for both candidates.
This model will establish Kafka (KRaft) as an evolution to its precursor (Kafka with ZooKeeper), explaining the problems it was designed to solve.
It will also define RabbitMQ's distinct architectural heritage.
Second, we will define our project's Problem Domain Habitat, the specific usage contexts and project needs, to determine which candidate is a better fit.

\subsubsection{Technology Domain Genealogy}
This model describes the ancestry and core design philosophy of the two competing technologies.
Candidate technology 1 is Apache Kafka (KRaft mode). Kafka is, as stated earlier, a distributed event streaming platform. Its core architecture is a persistent, immutable log.
It uses a dumb broker/smart consumer model, where the consumers pull data by tracking their position using an offset within the log.
The KRaft mode we are looking at is a significant evolution. Its precursor architecture required a separate, external Apache ZooKeeper cluster.
This external dependency was known to cause:\cite{kraft_vs_zookeeper}

1. High operational complexity: Requiring the management and security of two separate distributed systems.

2. Scalabiliy bottlenecks: Limiting clusters to tens of thousands of partitions.

3. Slow recovery times: Controller failover and metadata loading were slow.

KRaft replaces ZooKeeper by integrating a consensus protocol inside Kafka.
This was designed to provide simpler operations, massive scalability (to millions of partitions), and near instant, sub-second failover.

Candidate technology 2 is RabbitMQ.\@ RabbitMQ is a traditional message broker with a long heritage rooted in reliable delivey protocols like AMQP.\@
It functions as a smart broker/dumb consumer platform. The smart broker manages complex, flexible routing via exchanges and actively pushes messages to consumers.
Its core data structure is a queue. Messages are stateful and are typically deleted from the queue once successfully consumed and acknowledged.\cite{rabbitmq}

\subsubsection{Problem Domain Habitat}
As defined by the evaluation framework, the Problem Domain Habitat describes the specific usage contexts, problem characteristics, and key requirements where the candidate technologies will be deployed.\cite{brown:96}
Our habitat is the FeedApp backend, a real-time, interactive polling application built on Spring Boot and Java.
The primary objectives for this habitat are to find a messaging solution that is highly scalable, supports high throughput, and simultaneously provides good (low) latency.
These objetives can be split into two separate problem domain habitats.

\paragraph{Habitat 1: High-throughput event ingestion} The application must reliably ingest a high volume of events, such as poll creation, comments, and most critically votes from a potentially large and concurrent user base.
A user action is sent via the web API, processed, and then published as an immutable event by a producer service.
There are a few key requirements to this problem habitat.
We prioritize high throughput (handling thousands of writes per second) and scalability (the ability to add more nodes to handle increased load). 
The durability of these events is critical, as they effecitvely form the application's historical log.

\paragraph{Habitat 2: Real-time asynchronous fan-out} The application must feel responsive and as if event processing happens in real-time, as this is important for a good user experience.
When a poll-related event is successfully processed, the system must immediately notift all subscribed clients.
A consumer service listens for new events from a topic, it triggers a notification service, which pushes updates to clients via WebSockets.
In this habitat we prioritize low end-to-end latency. The time from an events publication to its consuption must be minimal for the application to feel responsive.

\subsection{Experiment Design}

Following the descriptive modeling phase, we now design the experiments to empirically evaulate our candidate technologies.
The goal of this phase is to formulate refutable hypotheses based on our problem domain habitat and design experiments to generate quantitative data to test them.

Our design focuses on getting objective data for our key requirements: throughput, latency and scalability.

\subsubsection{Hypothesis Formulation}
Our hypotheses are derived directly from the two habitats identified in the FeedApp backend and they key feature deltas between Kafka, which is a log based pull model, and RabbitMQ which is a queue based, push model.

Habitat 1 prioritizes high throughput and scalability.

Hypothesis 1.A (Throughput): In a many producers, high volume workload, Kafka will achieve significantly higher maximum throughput than RabbitMQ before latency thresholds are breached.
Here, throughput will be measured as messages per second.

Hypothesis 1.B (Scalability): As the number of producers increases, Kafka's throughput will scale better than RabbitMQ's.
Here, better scaling will be how much more load can be handled when increasing number of producers.

Habitat 2 prioritizes low end-to-end latency.

Hypothesis 2.A (Low-load latency): In a low message rate workload, RabbitMQ's push model will demonstrate lower p99 end-to-end latency than Kafka's pull model.

Hypothesis 2.B (High-load latency): As the message rate in this habitat increases, Kafka's pull model will become more efficient, and its average latency will become lower than RabbitMQ's, which will suffer from per-message overhead.

\subsubsection{Experiment Design and Workloads}
To test these hypotheses, we will use the OpenMessaging Benchmark (OMB) framework.
This provides a standardized, vendor-neutral toolset to conduct repeatable performance tests easily.
We will configure OMB with two distinct workloads, each designed as a model problem that simulates one of our defined habitats.

Workload 1 for Habitat 1: Ingestion test

The purpose of this workload is to test hypotheses 1.A and 1.B.
This will be the OMB workload configuration for this test.
\begin{lstlisting}[language=yaml, basicstyle=\ttfamily\small]
name: "workload1"
topics: 1
partitionsPerTopic: 3
messageSize: 100
useRandomizedPayloads: true
randomBytesRatio: 0.5
randomizedPayloadPoolSize: 1000
subscriptionsPerTopic: 1
consumerPerSubscription: 1
producersPerTopic: 100
producerRate: 500000
consumerBacklogSizeGB: 0
testDurationMinutes: 3
\end{lstlisting}
We will have many producers, a single topic/exchange, and a low number of consumers.
This simulates thousands of concurrent users casting votes and creating polls in the FeedApp.
There are three metrics we wish to capture in this test.

1. Maximum throughput: The highest sustainable message rate each platform can ingest

2. Producer latency (p99): The 99th percentile latency for a producer to successfully publish a message.

3. Scalability curve: Run the test with 1,5 and 10 producer instances to observe how throughput scales.

Workload 2 for Habitat 2: Fan-out test

The purpose of this workload is to test hypotheses 2.A and 2.B.
This will be the OMB workload configuration for the test.
\begin{lstlisting}[language=yaml, basicstyle=\ttfamily\small]
name: "workload2"
topics: 1
partitionsPerTopic: 3
messageSize: 1024 
useRandomizedPayloads: true
randomBytesRatio: 0.5
randomizedPayloadPoolSize: 1000
subscriptionsPerTopic: 70
consumerPerSubscription: 1
producersPerTopic: 1
producerRate: 10000 # This will vary, 100,1000,10000
consumerBacklogSizeGB: 0
testDurationMinutes: 3
\end{lstlisting}
We will have a single producer and many competing consumers on a shared subscription.
This simulates real time fanout of poll updates to all connected clients.
There are two metrics we wish to capture in this test:

1. End-to-end latency (p99): The time from message publish to message receipt by a consumer.

2. Latency vs. Throughput Curve: We will specifically run this workload at varying throughput rates (100 msg/sec, 1000 msg/sec and 10,000 msg/sec) to find the crossover point predicted in 2.B.

\subsection{Experiment Evaluation}

The experimental phase was aimed to gather empirical data to validate the suitability of the technologies for the \textbf{FeedApp} high volume ingestion (Habitat 1) and real time fan-out (Habitat 2) requirements.

\subsubsection{Evaluation of Workload 1: Ingestion Performance}
\begin{figure}[H]
    \centering
    % Timeline Stability
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload1_throughput.png}
        \caption{Throughput Stability over Time}
    	\label{fig:w1_throughput_time}
    \end{subfigure}
    \hfill
    % Bar Chart Comparison
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload1_throughput_comparison.png}
        \caption{Average Throughput Comparison}
        \label{fig:w1_throughput_bar}
    \end{subfigure}
    
    \caption{\textbf{Workload 1 Ingestion Performance.} Comparing the stability and raw average throughput of Kafka vs. RabbitMQ.}
    \label{fig:w1_throughput_group}
\end{figure}
Workload 1 simulated the \textbf{FeedApp} backend receiving a storm of user votes and poll creation events.
The primary metric for this habitat is throughput (messages/second and MB/s) and producer stability.
\paragraph{Throuput Analysis} As illustrated in Figure\ref{fig:w1_throughput_group}, the results strongly validate \textbf{Hypothesis 1.A}.
Apache Kafka demonstrated a significantly higher write capacity than RabbitMQ.\@
Looking at\ref{fig:w1_throughput_bar}, Kafka maintained a sustained average throughput of approximately 500,259 msg/s. In contrast, RabbitMQ saturated at 116,746 msg/s.
It is also important to mention that the producer rate was set at 500,000, which suggests that Kafka could reach even higher if this was increased.
Figure\ref{fig:w1_throughput_time} reveals the stability of this throughput. Kafka (blue line) maintains a near perfectly flat ingestion rate, indicating that the broker was not resource constrained by the producer load.
RabbitMQ (orange line), while relatively stable, operates at a much lower ceiling, suggesting it hit an internal processing bottleneck, likely the CPU cost of managing exchange routing and per-message persistence.
Figure\ref{fig:w1_throughput_time} also clearly illustrates the increased throughput capcaity (MB/s) between the two brokers.

\begin{figure}[H]
    \centering
    % Producer Latency
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload1_latency.png}
        \caption{Producer Publish Latency (p99)}
        \label{fig:w1_latency}
    \end{subfigure}
    \hfill
    % Consumer Backlog
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload1_backlog_comparison.png}
        \caption{Consumer Backlog Size (Log Scale)}
        \label{fig:w1_backlog}
    \end{subfigure}
    
    \caption{\textbf{Workload 1 System Stability.} Correlating producer latency with consumer backlog accumulation.}
    \label{fig:w1_stability_group}
\end{figure}

\paragraph{System Stability and Backlog} The cost of RabbitMQ's smart broker architecture becomes evident when analyzing latency and backlogs in Figure\ref{fig:w1_stability_group}.
Figure\ref{fig:w1_latency} shows the p99 public latency, which is the time it takes for the producer to get an acknowledgement from the broker.
Kafka consistently acknowledges writes in single-digit milliseconds. RabbitMQ, conversely, exihibits high volatility with p99 latencies oscillating between 500ms and 1,500ms.
This latency disparity is directly explained by~\ref{fig:w1_backlog}. RabbitMQ accumulates a significant consumer backlog, peaking over $10^{5}$ messages.
Because RabbitMQ stores messages in a queue structure that requires memory management and index updates for every message, the high ingestion rate cause the broker to slow down producers to allow consumers to catch up.
Kafka, utilizing a sequential append-only log, handled the backlog without impacting producer write latency, effectively decoupling the producer from the consumer.

\textbf{Conclusion on Habitat 1:} \textbf{Hypothesis 1.A is confirmed.}
In this experiment, Kafka provides approximately 4.2x the throughput of RabbitMQ, with significantly better stability.

\begin{figure}[H]
    \centering
    % Low and Medium Load
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload2_stability_100.png}
        \caption{Low Load (100 msg/s)}
        \label{fig:w2_100}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload2_stability_1000.png}
        \caption{Medium Load (1,000 msg/s)}
        \label{fig:w2_1000}
    \end{subfigure}
    
    \vspace{1em}
    
    % High Load
    \begin{subfigure}[b]{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload2_stability_10000.png}
        \caption{High Load (10,000 msg/s)}
        \label{fig:w2_10000}
    \end{subfigure}
    
    \caption{\textbf{Workload 2 Fanout Stability.} Evolution of end-to-end latency as throughput increases.}
    \label{fig:w2_stability_grid}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/workload2_latency_curve.png}
    \caption{\textbf{Latency vs. Throughput (p99).} Comparison of fanout latency scaling. Note the logarithmic scales on both axes.}
    \label{fig:w2_curve}
\end{figure}

\subsubsection{Evaluation of Workload 2: Fan-out Performance}
Workload 2 simulated the real time component of the app, pushing updates to clients. The primary metric here is end-to-end (E2E) latency.
\paragraph{Low vs. High Load Latency} The results for this workload yield unexpected findings regarding \textbf{Hypothesis 2.A}.
We hypothesized that RabbitMQ would offer lower latency at low loads due to its push based model.
However, the data refutes this. As seen in Figure\ref{fig:w2_100}, at a low load of 100 msg/s, Kafka maintains an E2E p99 latency of around 2ms.
RabbitMQ exhibits a consistent latency floor of 1000ms. The high baseline for RabbitMQ suggests a configuration interaction with the test harness, but it highlights that push does not automatically guarantee lower latency than pull if the persistence guarantees are strict.
\paragraph{Scalabiliy Under Load} Figure\ref{fig:w2_stability_grid} demonstrates the behavior as load increases to 10,000 msg/s (Figure\ref{fig:w2_10000}). Kafka remains stable with latencies under 20ms.
RabbitMQ's latency deteriorates further, spiking above 10,000ms.
The relationship is best summarized further by the logarithmic scale curve in Figure\ref{fig:w2_curve}.
Kafka's latency growth is linear and shallow, wheras RabbitMQ's latency growth is exponential relative to throughput.
\textbf{Conclusion on Habitat 2:} \textbf{Hypothesis 2.A is refuted} and \textbf{Hypothesis 2.B is confirmed}.
Even at low loads, Kafka's efficient zero-copy network transfer and sequential I/O provided lower latencies than RabbitMQ in this benchmark.
As load increased, Kafka's performance gap widened, proving it to be the more scalable solution for the \textbf{FeedApp's} fan-out requirements.
