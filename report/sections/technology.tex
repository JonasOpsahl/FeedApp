\section{Technology Assessment}
\label{sec:technology}

FeedApp utilizes Apache Kafka as a distributed event streaming platform to decouple write-intensive user actions (voting) from real-time notifications. Unlike traditional message queues that delete messages upon consumption, Kafka acts as a fault-tolerant, immutable log. 
To evaluate Kafka against its alternative, RabbitMQ, we adopt the software technology evaluation framework defined by Brown and Wallnau\cite{brown:96}. This framework guides our assessment through descriptive modeling of the technology's lineage (genealogy) and usage context (habitat), followed by hypothesis-driven experimentation.
To contextually ground the assessment, Table \ref{tab:kafka_concepts} defines the core streaming concepts implemented in the FeedApp architecture.

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{|l|p{10cm}|}
    \hline
    \textbf{Concept} & \textbf{Implementation in FeedApp} \\ \hline
    \textbf{Event} & An immutable record of a state change (e.g., a `Vote Cast') rather than a database update. \\ \hline
    \textbf{Topic} & A category for event storage. We utilize a \textit{dynamic topic} strategy where every unique poll gets a dedicated topic (e.g., \texttt{poll.voteChange.10}) to isolate event streams. \\ \hline
    \textbf{Consumer Group} & A configuration (\texttt{poll-app}) that allows multiple backend instances to share the load of processing events. \\ \hline
    \textbf{Cluster} & A three-node broker setup using the KRaft consensus protocol (removing ZooKeeper) to ensure fault tolerance and high availability. \\ \hline
    \end{tabular}
    \caption{Key Messaging Concepts and Configuration in FeedApp}
    \label{tab:kafka_concepts}
\end{table}

This part of the report aims to follow this software technology evaluation framework~\ref{fig:framework}.


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figs/framework.png}
	\caption{Software technology evaluation framework.}
	\label{fig:framework}
\end{figure}

\subsection{Descriptive Modeling}

The purpose of this phase is to establish a formal model for evaluating two candidate technologies, Apache Kafka (in KRaft mode) and RabbitMQ for adoption within our project.
First, we will define the Technology Domain Genealogy for both candidates.
This model will establish Kafka (KRaft) as an evolution to its precursor (Kafka with ZooKeeper), explaining the problems it was designed to solve.
It will also define RabbitMQ's distinct architectural heritage.
Second, we will define our project's Problem Domain Habitat, the specific usage contexts and project needs, to determine which candidate is a better fit.

\subsubsection{Technology Domain Genealogy}
This model describes the ancestry and core design philosophy of the two competing technologies.
Candidate technology 1 is Apache Kafka (KRaft mode). Kafka is, as stated earlier, a distributed event streaming platform. Its core architecture is a persistent, immutable log.
It uses a dumb broker/smart consumer model, where the consumers pull data by tracking their position using an offset within the log.
The KRaft mode we are looking at is a significant evolution. Its precursor architecture required a separate, external Apache ZooKeeper cluster.
This external dependency was known to cause\cite{kraft_vs_zookeeper}:

1. High operational complexity: Requiring the management and security of two separate distributed systems.

2. Scalabiliy bottlenecks: Limiting clusters to tens of thousands of partitions.

3. Slow recovery times: Controller failover and metadata loading were slow.

KRaft replaces ZooKeeper by integrating a consensus protocol inside Kafka.
This was designed to provide simpler operations, massive scalability (to millions of partitions), and near instant, sub-second failover.

Candidate technology 2 is RabbitMQ.\@ RabbitMQ is a traditional message broker with a long heritage rooted in reliable delivey protocols like AMQP.\@
It functions as a smart broker/dumb consumer platform. The smart broker manages complex, flexible routing via exchanges and actively pushes messages to consumers.
Its core data structure is a queue. Messages are stateful and are typically deleted from the queue once successfully consumed and acknowledged\cite{rabbitmq}.

\subsubsection{Problem Domain Habitat}
As defined by the evaluation framework, the Problem Domain Habitat describes the specific usage contexts, problem characteristics, and key requirements where the candidate technologies will be deployed\cite{brown:96}.
Our habitat is the FeedApp backend, a real-time, interactive polling application built on Spring Boot and Java.
The primary objectives for this habitat are to find a messaging solution that is highly scalable, supports high throughput, and simultaneously provides good (low) latency.
These objetives can be split into two separate problem domain habitats.

\paragraph{Habitat 1: High-throughput event ingestion} The application must reliably ingest a high volume of events, such as poll creation, comments, and most critically votes from a potentially large and concurrent user base.
A user action is sent via the web API, processed, and then published as an immutable event by a producer service.
There are a few key requirements to this problem habitat.
We prioritize high throughput (handling thousands of writes per second) and scalability (the ability to add more nodes to handle increased load). 
The durability of these events is critical, as they effecitvely form the application's historical log.

\paragraph{Habitat 2: Real-time asynchronous fan-out} The application must feel responsive and as if event processing happens in real-time, as this is important for a good user experience.
When a poll-related event is successfully processed, the system must immediately notift all subscribed clients.
A consumer service listens for new events from a topic, it triggers a notification service, which pushes updates to clients via WebSockets.
In this habitat we prioritize low end-to-end latency. The time from an events publication to its consuption must be minimal for the application to feel responsive.

\subsection{Experiment Design}

Following the descriptive modeling phase, we now design the experiments to empirically evaulate our candidate technologies.
The goal of this phase is to formulate refutable hypotheses based on our problem domain habitat and design experiments to generate quantitative data to test them.

Our design focuses on getting objective data for our key requirements: throughput, latency and scalability.

\subsubsection{Hypothesis Formulation}
Our hypotheses are derived directly from the two habitats identified in the FeedApp backend and they key feature deltas between Kafka, which is a log based pull model, and RabbitMQ which is a queue based, push model.

Habitat 1 prioritizes high throughput and scalability.

Hypothesis 1.A (Throughput): In a many producers, high volume workload, Kafka will achieve significantly higher maximum throughput than RabbitMQ before latency thresholds are breached.
Here, throughput will be measured as messages per second.

Hypothesis 1.B (Scalability): As the number of producers increases, Kafka's throughput will scale better than RabbitMQ's.
Here, better scaling will be how much more load can be handled when increasing number of producers.

Habitat 2 prioritizes low end-to-end latency.

Hypothesis 2.A (Low-load latency): In a low message rate workload, RabbitMQ's push model will demonstrate lower p99 end-to-end latency than Kafka's pull model.

Hypothesis 2.B (High-load latency): As the message rate in this habitat increases, Kafka's pull model will become more efficient, and its average latency will become lower than RabbitMQ's, which will suffer from per-message overhead.

\subsubsection{Experiment Design and Workloads}
To test these hypotheses, we will use the OpenMessaging Benchmark (OMB) framework.
This provides a standardized, vendor-neutral toolset to conduct repeatable performance tests easily.
We will configure OMB with two distinct workloads, each designed as a model problem that simulates one of our defined habitats.

Workload 1 for Habitat 1: Ingestion test

The purpose of this workload is to test hypotheses 1.A and 1.B.
To simulate high-volume ingestion (Habitat 1), we configured the benchmark with 100 producers generating small 100-byte messages at a target rate of 500,000 messages per second.
The test utilized a single topic with three partitions to isolate raw ingestion throughput and producer stability.
There are a few things we wish to look at with this workload:

1. Maximum throughput: The highest sustainable message rate each platform can ingest

2. Producer latency (p99): The 99th percentile latency for a producer to successfully publish a message.

3. Scalability curve: How good is the throughput with a lot of producers.

Workload 2 for Habitat 2: Fan-out test

The purpose of this workload is to test hypotheses 2.A and 2.B.
For the fan-out scenario (Habitat 2), the workload used a single producer emitting larger 1KB messages to a topic with 70 competing consumers, simulating a broadcast to many connected clients.
We executed this test at varying throughput rates (100, 1,000, and 10,000 msg/s) to identify the `crossover point' where latency degradation begins.
There are two metrics we wish to capture in this test:

1. End-to-end latency (p99): The time from message publish to message receipt by a consumer.

2. Latency vs. Throughput Curve: We will specifically run this workload at varying throughput rates (100 msg/sec, 1000 msg/sec and 10,000 msg/sec) to find the crossover point predicted in 2.B.

\subsection{Experiment Evaluation}

The experimental phase was aimed to gather empirical data to validate the suitability of the technologies for the \textbf{FeedApp} high volume ingestion (Habitat 1) and real time fan-out (Habitat 2) requirements.

\subsubsection{Evaluation of Workload 1: Ingestion Performance}
\begin{figure}[H]
    \centering
    % Timeline Stability
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload1_throughput.png}
        \caption{Throughput Stability over Time}
    	\label{fig:w1_throughput_time}
    \end{subfigure}
    \hfill
    % Bar Chart Comparison
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload1_throughput_comparison.png}
        \caption{Average Throughput Comparison}
        \label{fig:w1_throughput_bar}
    \end{subfigure}
    
    \caption{{Workload 1 Ingestion Performance.}}
    \label{fig:w1_throughput_group}
\end{figure}
Workload 1 simulated the \textbf{FeedApp} backend receiving a storm of user votes and poll creation events.
The primary metric for this habitat is throughput (messages/second and MB/s) and producer stability.
\paragraph{Throuput Analysis} As illustrated in Figure~\ref{fig:w1_throughput_group}, the results strongly validate \textbf{Hypothesis 1.A}.
Apache Kafka demonstrated a significantly higher write capacity than RabbitMQ.\@
Looking at~\ref{fig:w1_throughput_bar}, Kafka maintained a sustained average throughput of approximately 500,259 msg/s. In contrast, RabbitMQ saturated at 116,746 msg/s.
It is also important to mention that the producer rate was set at 500,000, which suggests that Kafka could reach even higher if this was increased.
Figure~\ref{fig:w1_throughput_time} reveals the stability of this throughput. Kafka (blue line) maintains a near perfectly flat ingestion rate, indicating that the broker was not resource constrained by the producer load.
RabbitMQ (orange line), while relatively stable, operates at a much lower ceiling, suggesting it hit an internal processing bottleneck, likely the CPU cost of managing exchange routing and per-message persistence.
Figure~\ref{fig:w1_throughput_time} also clearly illustrates the increased throughput capcaity (MB/s) between the two brokers.

\begin{figure}[H]
    \centering
    % Producer Latency
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload1_latency.png}
        \caption{Producer Publish Latency (p99)}
        \label{fig:w1_latency}
    \end{subfigure}
    \hfill
    % Consumer Backlog
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload1_backlog_comparison.png}
        \caption{Consumer Backlog Size (Log Scale)}
        \label{fig:w1_backlog}
    \end{subfigure}
    
    \caption{{Workload 1 System Stability.}}
    \label{fig:w1_stability_group}
\end{figure}

\paragraph{System Stability and Backlog} The cost of RabbitMQ's smart broker architecture becomes evident when analyzing latency and backlogs in Figure~\ref{fig:w1_stability_group}.
Figure~\ref{fig:w1_latency} shows the p99 public latency, which is the time it takes for the producer to get an acknowledgement from the broker.
Kafka consistently acknowledges writes in single-digit milliseconds. RabbitMQ, conversely, exihibits high volatility with p99 latencies oscillating between 500ms and 1,500ms.
This latency disparity is directly explained by~\ref{fig:w1_backlog}. RabbitMQ accumulates a significant consumer backlog, peaking over $10^{5}$ messages.
Because RabbitMQ stores messages in a queue structure that requires memory management and index updates for every message, the high ingestion rate cause the broker to slow down producers to allow consumers to catch up.
Kafka, utilizing a sequential append-only log, handled the backlog without impacting producer write latency, effectively decoupling the producer from the consumer.

\textbf{Conclusion on Habitat 1:} \textbf{Hypothesis 1.A is confirmed.}
In this experiment, Kafka provides approximately 4.2x the throughput of RabbitMQ, with significantly better stability.

\begin{figure}[H]
    \centering
    % Low and Medium Load
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload2_stability_100.png}
        \caption{Low Load (100 msg/s)}
        \label{fig:w2_100}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload2_stability_1000.png}
        \caption{Medium Load (1,000 msg/s)}
        \label{fig:w2_1000}
    \end{subfigure}
    
    \vspace{1em}
    
    % High Load
    \begin{subfigure}[b]{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/workload2_stability_10000.png}
        \caption{High Load (10,000 msg/s)}
        \label{fig:w2_10000}
    \end{subfigure}
    
    \caption{{Workload 2 Fanout Stability.}}
    \label{fig:w2_stability_grid}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figs/workload2_latency_curve.png}
    \caption{{Latency vs. Throughput (p99).}}
    \label{fig:w2_curve}
\end{figure}

\subsubsection{Evaluation of Workload 2: Fan-out Performance}
Workload 2 simulated the real time component of the app, pushing updates to clients. The primary metric here is end-to-end (E2E) latency.
\paragraph{Low vs. High Load Latency} The results for this workload yield unexpected findings regarding \textbf{Hypothesis 2.A}.
We hypothesized that RabbitMQ would offer lower latency at low loads due to its push based model.
However, the data refutes this. As seen in Figure~\ref{fig:w2_100}, at a low load of 100 msg/s, Kafka maintains an E2E p99 latency of around 2ms.
RabbitMQ exhibits a consistent latency floor of 1000ms. The high baseline for RabbitMQ suggests a configuration interaction with the test harness, but it highlights that push does not automatically guarantee lower latency than pull if the persistence guarantees are strict.
\paragraph{Scalabiliy Under Load} Figure~\ref{fig:w2_stability_grid} demonstrates the behavior as load increases to 10,000 msg/s (Figure~\ref{fig:w2_10000}). Kafka remains stable with latencies under 20ms.
RabbitMQ's latency deteriorates further, spiking above 10,000ms.
The relationship is best summarized further by the logarithmic scale curve in Figure~\ref{fig:w2_curve}.
Kafka's latency growth is linear and shallow, wheras RabbitMQ's latency growth is exponential relative to throughput.
\textbf{Conclusion on Habitat 2:} \textbf{Hypothesis 2.A is refuted} and \textbf{Hypothesis 2.B is confirmed}.
Even at low loads, Kafka's efficient zero-copy network transfer and sequential I/O provided lower latencies than RabbitMQ in this benchmark\cite{kafka_fast}.
As load increased, Kafka's performance gap widened, proving it to be the more scalable solution for the \textbf{FeedApp's} fan-out requirements.
