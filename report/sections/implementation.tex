\section{Prototype Implementation}
\label{sec:implementation}
This section gives a short overview of how the prototype is set up, how the backend processes changes, 
how updates reach the clients, and what developers can build on further. The focus here is on the two 
technologies that drive the real-time behaviour in the system: Kafka and WebSockets.
\subsection{Setup and Deployment}
The full system runs in Docker Compose, including the backend, frontend, Kafka (3 brokers) and Redis.
Developers do not need Java or Kafka installed locally, everything is started with:
\begin{lstlisting}
docker compose up -d
\end{lstlisting}
Once running, the frontend is available at \texttt{http://localhost:5173}, and it talks to the backend and 
WebSocket server automatically inside the Docker network. We also built and published Docker images for both the backend 
and frontend, and the links to these images are included below.
To automate this, we created a small GitHub Actions workflow that builds the project, starts the containers, 
and runs health checks on every push or pull request to \texttt{"main"}.
\subsection{Backend}
The backend is a Spring Boot application. Controllers expose REST endpoints, services handle the logic, 
and JPA repositories use Hibernate as the ORM layer against an H2 database during development. Poll results are cached in Redis to avoid 
running the same SQL queries repeatedly.
The interesting part is how a change moves through the system: when someone votes or comments, the backend saves it, 
sends a small event to Kafka, the consumer reacts by clearing the cache, and finally a WebSocket message notifies all connected clients.
This lets the UI update instantly without reloading the page or spamming the server with requests.
We added a simple security setup using JWT tokens, so users can log in and the backend can protect the routes that require authentication.
\subsubsection*{Kafka Integration}
Each poll gets its own topic so vote ordering is kept local (\texttt{poll.voteChange.<id>}). When a change happens, 
the backend publishes a small event:
\begin{lstlisting}[language=Java]
// ProducerService
public void sendEvent(String topic, Map<String,Object> data) {
kafkaTemplate.send(topic, data);
}
\end{lstlisting}
When a new poll is created, we also create a new Kafka topic with a single partition, replicated on three brokers, 
so messages remain ordered and survive a node failure.
\begin{lstlisting}[language=Java]
// PollTopicManager
NewTopic topic = new NewTopic("poll.voteChange." + pollId, 1, (short) 3);
kafkaAdmin.createOrModifyTopics(topic);
\end{lstlisting}
A single Kafka consumer listens to all poll topics and reacts to events:
\begin{lstlisting}[language=Java]
// ConsumerService
@KafkaListener(topicPattern="poll.voteChange.*")
public void consumeVoteChangeEvent(Map<String,Object> data) {
pollService.invalidatePollCache((Integer) data.get("pollId"));
RawWebSocketServer.broadcastJson(Map.of(
"type","vote-delta",
"pollId", data.get("pollId")
));
}
\end{lstlisting}
This keeps the backend easy to deal with. Kafka takes care of the ordering for us, and we only send the clients the small updates they need.
\subsection{WebSockets}
Instead of sending all the poll data through WebSockets, the server just sends small signals. 
When something changes, the backend calls:
\begin{lstlisting}[language=Java]
RawWebSocketServer.broadcast("pollsUpdated");
\end{lstlisting}
or when sending a JSON message:
\begin{lstlisting}[language=Java]
RawWebSocketServer.broadcastJson(Map.of("type","poll-created"));
\end{lstlisting}
These WebSocket signals are triggered directly in the controller methods that handle actions such as creating polls, deleting polls, or adding comments.
On the frontend, there is only one WebSocket connection for the whole app. When a message arrives, the client decides what to reload:
\begin{lstlisting}[language=TypeScript]
onWs(msg => { // App.tsx
if (msg === "pollsUpdated") refreshPolls();
});
\end{lstlisting}
For more detailed updates (like comments or vote deltas), the frontend receives a small JSON message, 
checks the type and only refreshes the parts that changed.
The WebSocket server itself is simple, it keeps track of open sessions and has a couple of helper 
methods for sending messages to all clients. This keeps the communication consistent and easy to extend.
\subsection{Frontend}
The React + TypeScript frontend fetches poll data over REST, while a single WebSocket connection listens for updates.
Because of this, the UI reacts instantly when someone votes or comments, without reloading the page.
The frontend is kept simple by splitting everything into components (CreatePoll, VoteOnPoll, CommentSection), 
and \texttt{"App.tsx"} wires it all together using the small WebSocket helper in \texttt{"ws.ts"}.
\subsection{Extensibility}
Because the system is modular, itâ€™s easy to add new features, such as new poll 
types using the same event flow, mobile apps using the same API and WebSocket, 
extra Kafka event types, or stronger authentication and sharing of private polls with friends. We planned to create our own external benchmarks, but due to time constraints we were not able to run them.